{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5a31d058a6316b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业一：实现HMM中文分词和BPE英文分词\n",
    "姓名：邵言\n",
    "\n",
    "学号：523031910224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a16ed",
   "metadata": {},
   "source": [
    "## 任务一：HMM模型用于中文分词\n",
    "\n",
    "任务一评分标准：\n",
    "1. 共有8处TODO需要填写，共10分。\n",
    "2. **可编辑代码区域仅限定在TODO的范围内，不允许自行修改其他部分代码。**\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5fc4dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77db9",
   "metadata": {},
   "source": [
    "导入HMM参数，初始化所需的起始概率矩阵、转移概率矩阵、发射概率矩阵，并将它们转换为<b>对数形式</b>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0d25beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z0/c0f0lkj135z7bz51zvc4b5q00000gn/T/ipykernel_2005/4197557067.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_matrix_log = np.log(hmm_parameters[\"emission_mat\"])  # shape(2, 65536)\n"
     ]
    }
   ],
   "source": [
    "with open(\"hmm_parameters.pkl\", \"rb\") as f:\n",
    "    hmm_parameters = pickle.load(f)\n",
    "\n",
    "# 非断字（B）为第0行，断字（I）为第1行\n",
    "# 发射概率矩阵中，词典大小为65536，以汉字的Unicode码点（一个整数值）作为行索引\n",
    "start_prob_log = np.log(hmm_parameters[\"start_prob\"])  # shape(2,)\n",
    "trans_matrix_log = np.log(hmm_parameters[\"trans_mat\"])  # shape(2, 2)\n",
    "emission_matrix_log = np.log(hmm_parameters[\"emission_mat\"])  # shape(2, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070152",
   "metadata": {},
   "source": [
    "定义待处理的句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "87219e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 将your_name中的xxx替换为你的姓名\n",
    "your_name = \"邵言\"\n",
    "\n",
    "input_sentence = f\"{your_name}是一名优秀的学生\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035cbc7",
   "metadata": {},
   "source": [
    "### 1.1 实现Viterbi算法\n",
    "实现Viterbi算法，并以此进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1adac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sent_orig: str, start_prob_log: np.ndarray, trans_mat_log: np.ndarray, emission_mat_log: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Viterbi算法进行中文分词。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob_log: numpy.ndarray - 起始对数概率矩阵\n",
    "        trans_mat_log: numpy.ndarray - 转移对数概率矩阵\n",
    "        emission_mat_log: numpy.ndarray - 发射对数概率矩阵\n",
    "\n",
    "    Return:\n",
    "        str - 中文分词的结果\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种标注（B/I）的最大对数概率值\n",
    "    dp = np.empty((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # `path`用来储存最大概率对应的上步B/I选择\n",
    "    #  例如 path[1][7] == 1 意味着第8个（从1开始计数）字符标注I对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[0][5] == 1 意味着第6个字符标注B对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[1][1] == 0 意味着第2个字符标注I对应的最大概率，其前一步的隐状态为0（B）\n",
    "    path = np.zeros((2, len(sent_ord)), dtype=int)\n",
    "\n",
    "    # [1] TODO: 第一个位置的最大概率值计算【1分】 =================>>>\n",
    "    dp[0,0] = start_prob_log[0] + emission_matrix_log[0,sent_ord[0]]   # 第一个为0\n",
    "    dp[1,0] = start_prob_log[1] + emission_matrix_log[1,sent_ord[0]]   # 第一个为1\n",
    "\n",
    "    # [1] <<<======================= END ==========================\n",
    "\n",
    "    # [2] TODO: 其余位置的最大概率值计算（填充dp和path矩阵）【2分】 =====>>>\n",
    "    for i in range(len(sent_ord)-1):\n",
    "        # 计算转移的条件概率\n",
    "        prob_trans = np.array([[dp[0,i] + trans_matrix_log[0,0], dp[0,i] + trans_matrix_log[0,1]],\n",
    "                      [dp[1,i] + trans_matrix_log[1,0], dp[1,i] + trans_matrix_log[1,1]]])\n",
    "\n",
    "        # argmax，更新path\n",
    "        if ( prob_trans[0,0] > prob_trans[1,0] ):\n",
    "            path[0,i+1] = 0\n",
    "            base_0 = prob_trans[0,0]\n",
    "        else:\n",
    "            path[0,i+1] = 1\n",
    "            base_0 = prob_trans[1,0]\n",
    "        if ( prob_trans[0,1] > prob_trans[1,1] ):\n",
    "            path[1,i+1] = 0\n",
    "            base_1 = prob_trans[0,1]\n",
    "        else:\n",
    "            path[1,i+1] = 1\n",
    "            base_1 = prob_trans[1,1]\n",
    "\n",
    "        # 更新dp\n",
    "        dp[0,i+1] = base_0 + emission_matrix_log[0,sent_ord[i+1]]\n",
    "        dp[1,i+1] = base_1 + emission_matrix_log[1,sent_ord[i+1]]\n",
    "\n",
    "    # [2] <<<======================= END ==========================\n",
    "\n",
    "    #  `labels`用来储存每个位置最有可能的隐状态\n",
    "    labels = [0 for _ in range(len(sent_ord))]\n",
    "\n",
    "    # [3] TODO: 计算labels每个位置上的值（填充labels矩阵）【1分】 ====>>>\n",
    "    current = len(sent_ord) - 1\n",
    "    labels[current] = 0 if dp[0,current] > dp[1,current] else 1\n",
    "    while current > 0:\n",
    "        labels[current-1] = path[labels[current],current]\n",
    "        current -= 1\n",
    "\n",
    "    # [3] <<<======================= END ==========================\n",
    "\n",
    "    #  根据labels生成切分好的字符串\n",
    "    sent_split = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            sent_split += [sent_ord[idx], ord(\"/\")]\n",
    "        else:\n",
    "            sent_split += [sent_ord[idx]]\n",
    "    sent_split_str = \"\".join([chr(x) for x in sent_split])\n",
    "\n",
    "    return sent_split_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b7bd0",
   "metadata": {},
   "source": [
    "你可以用下述句子测试Viterbi算法的实现：若实现正确，则输出结果应该是`我/爱上/海交/通大/学的/自然/语言/处理/`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fd367283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi算法分词结果： 我/爱上/海交/通大/学的/自然/语言/处理/\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"我爱上海交通大学的自然语言处理\"\n",
    "print(\"Viterbi算法分词结果：\", viterbi(test_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc86ae2",
   "metadata": {},
   "source": [
    "检查无误后运行下方单元格，对`input_sentence`做分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d795414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi算法分词结果： 邵言/是/一名/优秀/的/学生/\n"
     ]
    }
   ],
   "source": [
    "print(\"Viterbi算法分词结果：\", viterbi(input_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcafdb",
   "metadata": {},
   "source": [
    "### 1.2 实现前向算法\n",
    "实现前向算法，计算该句子的对数概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cf6796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob_by_forward(sent_orig: str, start_prob_log: np.ndarray, trans_mat_log: np.ndarray, emission_mat_log: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    前向算法，计算输入中文句子的对数概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob_log: numpy.ndarray - 起始对数概率矩阵\n",
    "        trans_mat_log: numpy.ndarray - 转移对数概率矩阵\n",
    "        emission_mat_log: numpy.ndarray - 发射对数概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 对数概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，到该位置为止的句子的对数概率\n",
    "    dp = np.empty((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # [1] TODO: 初始位置概率的计算【1分】 ==========================>>>\n",
    "    dp[0,0] = start_prob_log[0] + emission_matrix_log[0,sent_ord[0]]    # 第一个为0\n",
    "    dp[1,0] = start_prob_log[1] + emission_matrix_log[1,sent_ord[0]]    # 第一个为1\n",
    "    # [1] <<<======================= END ==========================\n",
    "\n",
    "    ans = None\n",
    "    # [2] TODO: 先计算其余位置的概率（填充dp矩阵），然后返回对数概率值【2分】 =====>>>\n",
    "    for i in range(len(sent_ord)-1):\n",
    "        dp[0,i+1] = np.log(np.exp(dp[0,i] + trans_matrix_log[0,0]) + np.exp(dp[1,i] + trans_matrix_log[1,0])) + emission_matrix_log[0,sent_ord[i+1]]\n",
    "        dp[1,i+1] = np.log(np.exp(dp[0,i] + trans_matrix_log[0,1]) + np.exp(dp[1,i] + trans_matrix_log[1,1])) + emission_matrix_log[1,sent_ord[i+1]]\n",
    "    ans = np.log(np.sum(np.exp(dp[:,-1])))\n",
    "    # [2] <<<======================= END ==========================\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59533cd8",
   "metadata": {},
   "source": [
    "### 1.3 实现后向算法\n",
    "实现后向算法，计算该句子的对数概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1e898306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob_by_backward(sent_orig: str, start_prob_log: np.ndarray, trans_mat_log: np.ndarray, emission_mat_log: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    后向算法，计算输入中文句子的对数概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob_log: numpy.ndarray - 起始对数概率矩阵\n",
    "        trans_mat_log: numpy.ndarray - 转移对数概率矩阵\n",
    "        emission_mat_log: numpy.ndarray - 发射对数概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 对数概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，从结尾到该位置为止的句子的概率\n",
    "    dp = np.empty((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # [1] TODO: 终末位置概率的初始化【1分】 =================>>>\n",
    "    dp[0,-1] = 0   # 倒数第一个为0\n",
    "    dp[1,-1] = 0   # 倒数第一个为1\n",
    "\n",
    "    # [1] <<<======================= END =====================\n",
    "\n",
    "    ans = None\n",
    "    # [2] TODO: 先计算其余位置的概率（填充dp矩阵），然后返回概率值【2分】 =====>>>\n",
    "    for i in range(len(sent_ord)-1,0,-1):\n",
    "        dp[0, i-1] = np.log(np.exp(trans_matrix_log[0,0] + emission_matrix_log[0,sent_ord[i]] + dp[0,i]) + np.exp(trans_matrix_log[0,1] + emission_matrix_log[1,sent_ord[i]] + dp[1,i]))\n",
    "        dp[1, i-1] = np.log(np.exp(trans_matrix_log[1,0] + emission_matrix_log[0,sent_ord[i]] + dp[0,i]) + np.exp(trans_matrix_log[1,1] + emission_matrix_log[1,sent_ord[i]] + dp[1,i]))\n",
    "\n",
    "    ans = np.log(np.exp(start_prob_log[0] + emission_matrix_log[0,sent_ord[0]] + dp[0,0]) + np.exp(start_prob_log[1] + emission_matrix_log[1,sent_ord[0]] + dp[1,0]))\n",
    "\n",
    "    # [2] <<<======================= END ==========================\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209660a",
   "metadata": {},
   "source": [
    "如果前向算法与后向算法的实现正确，下面的测试句子所给出的两种算法概率应当几乎相等，约为`-99.9266`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "33378f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向算法概率： -99.92661770504658\n",
      "后向算法概率： -99.92661770504657\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"我爱上海交通大学的自然语言处理\"\n",
    "print(\"前向算法概率：\", compute_logprob_by_forward(test_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))\n",
    "print(\"后向算法概率：\", compute_logprob_by_backward(test_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a386f49",
   "metadata": {},
   "source": [
    "现在计算`input_sentence`的句子概率值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b26101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向算法概率： -67.61320986497117\n",
      "后向算法概率： -67.61320986497115\n"
     ]
    }
   ],
   "source": [
    "print(\"前向算法概率：\", compute_logprob_by_forward(input_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))\n",
    "print(\"后向算法概率：\", compute_logprob_by_backward(input_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49866453",
   "metadata": {},
   "source": [
    "> 如果你的名字含有生僻字，分词结果以及计算出的句子概率值可能会很“奇怪”。思考一下这是为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803f6a1a465dd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 实验总结\n",
    "> TODO：请在这里填写实验总结。\n",
    "- Viterbi算法：找出最佳路径。每一步找出应该走哪一步从上个状态到达当前状态。\n",
    "- 前向算法与后向算法。把上一步的所有情况当作一个整体，计算下一步各种可能性的概率分布。\n",
    "    - 注意前向算法和后向算法逻辑上的不同但是结果上的相似，但是实现上略有不同：\n",
    "        - 前向算法从前开始初始化，往后计算概率，在最后读取结果。每一个状态的概率依赖于上一个状态的两种情况。\n",
    "        - 后向算法从最后开始初始化（因此初始化为log(1)=0，因为已经是最终的结果，是确定值），往前计算概率，在开头读取结果（还要乘以start_prob的权重）。每一个状态的概率取决于下一个状态的两种情况。\n",
    "\n",
    "要注意概率和概率对数的正确转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994be6e",
   "metadata": {},
   "source": [
    "## 任务二：BPE算法用于英文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc4775",
   "metadata": {},
   "source": [
    "任务二评分标准：\n",
    "\n",
    "1. 共有6处TODO需要填写，共10分。\n",
    "2. **可编辑代码区域仅限定在TODO的范围内，不允许自行修改其他部分代码。**\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f02463b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3951dc",
   "metadata": {},
   "source": [
    "导入英语语料库数据。利用这些自然语料，我们将构建一个BPE分词器，对输入文本进行tokenize。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "23ff301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The journey from the Clyde to Inverness was promoted as the Royal Route after Queen Victoria and Prince Albert made the trip in 1847 and the monarch also sailed on the Gondolier in 1873.\n",
      "The warning from the Fed, which cut interest rates earlier this week, triggered concern that it might hold off further rate cuts or even consider raising them if inflation accelerates.\n",
      "When the witness' boyfriend tried to seek help from security, he was jumped by the posse of Smith's boyfriend and left with a fractured face.\n",
      "There are concrete limits to growth and no one wants to admit that.\n",
      "KABUL, Afghanistan (CNN) -- Ten people -- including three members of Afghanistan's parliament -- were killed in a suicide bomb blast Tuesday as they visited a sugar plant in Baghlan province, another member of parliament told CNN.\n"
     ]
    }
   ],
   "source": [
    "with open(\"news_2007_en.txt\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()))     # List[str]\n",
    "for line in training_corpus[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732502a",
   "metadata": {},
   "source": [
    "### 2.1 构建单词频次字典\n",
    "首先将语料中的句子以空格切分成单词，然后将单词拆分成字母加`</w>`的形式，例如`apple`将变为`a p p l e </w>`。请编写相应函数构建BPE算法需要用到的初始状态词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7bf823e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "def white_space_tokenize(corpus: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    先正则化（字母转小写、数字转为N、除去标点符号），然后以空格分词语料中的句子，例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到 `[[\"i\", \"am\", \"happy\"], [\"i\", \"have\", \"N\", \"apples\"]]`\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        List[List[str]] - 二维List，内部的List由每个句子的单词str构成\n",
    "\n",
    "    【辅助函数，该函数无需改动】\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [list(filter(lambda token: len(token) > 0, _splitor_pattern.split(_digit_pattern.sub(\"N\", sentence.lower())))) for sentence in corpus]\n",
    "    return tokeneds\n",
    "\n",
    "def build_bpe_vocab(corpus: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    获取语料库中的所有单词，将单词每个字母以空格隔开、结尾加上</w>（单词终止符）后，构建带频数的字典。例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = white_space_tokenize(corpus)\n",
    "\n",
    "    bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体【2分】 =============================>>>\n",
    "    for sentence in tokenized_corpus:\n",
    "        for word in sentence:\n",
    "            w = ' '.join(word) + ' </w>'\n",
    "            bpe_vocab[w] = bpe_vocab.get(w, 0) + 1\n",
    "\n",
    "    # <<<======================= END ========================\n",
    "\n",
    "    return bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1a932",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "02cce7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i </w>': 2,\n",
       " 'a m </w>': 1,\n",
       " 'h a p p y </w>': 1,\n",
       " 'h a v e </w>': 1,\n",
       " 'N </w>': 1,\n",
       " 'a p p l e s </w>': 1}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bpe_vocab = build_bpe_vocab([\"I am happy.\", \"I have 10 apples!\"])\n",
    "test_bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d25245",
   "metadata": {},
   "source": [
    "### 2.2 构建bigram频次字典\n",
    "单词频次字典中，每个键都是由空格分隔开的unigram组成的字符串。请统计这些unigram所组成的所有bigram的频次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "087d11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_freq(bpe_vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    统计\"单词分词状态->频数\"的词典中，各bigram的频次（假设该词典中，各个unigram以空格间隔），例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        ('i', '</w>'): 2,\n",
    "        ('a', 'm'): 1,\n",
    "        ('m', '</w>'): 1,\n",
    "        ('h', 'a'): 2,\n",
    "        ('a', 'p'): 2,\n",
    "        ('p', 'p'): 2,\n",
    "        ('p', 'y'): 1,\n",
    "        ('y', '</w>'): 1,\n",
    "        ('a', 'v'): 1,\n",
    "        ('v', 'e'): 1,\n",
    "        ('e', '</w>'): 1,\n",
    "        ('N', '</w>'): 1,\n",
    "        ('p', 'l'): 1,\n",
    "        ('l', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', '</w>'): 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[Tuple[str, str], int] - \"bigram->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_freq = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】 =============================>>>\n",
    "    for item, count in bpe_vocab.items():\n",
    "        characters = item.split(\" \")\n",
    "        for i in range(len(characters)-1):\n",
    "            bigram_freq[(characters[i],characters[i+1])] = bigram_freq.get((characters[i],characters[i+1]),0) + count\n",
    "    # <<<======================= END ========================\n",
    "\n",
    "    return bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1ff1b",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f7db7349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('i', '</w>'): 2,\n",
       " ('a', 'm'): 1,\n",
       " ('m', '</w>'): 1,\n",
       " ('h', 'a'): 2,\n",
       " ('a', 'p'): 2,\n",
       " ('p', 'p'): 2,\n",
       " ('p', 'y'): 1,\n",
       " ('y', '</w>'): 1,\n",
       " ('a', 'v'): 1,\n",
       " ('v', 'e'): 1,\n",
       " ('e', '</w>'): 1,\n",
       " ('N', '</w>'): 1,\n",
       " ('p', 'l'): 1,\n",
       " ('l', 'e'): 1,\n",
       " ('e', 's'): 1,\n",
       " ('s', '</w>'): 1}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bigram_freq = get_bigram_freq(test_bpe_vocab)\n",
    "test_bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d313f",
   "metadata": {},
   "source": [
    "### 2.3 合并bigram\n",
    "BPE算法的每轮迭代都会合并出现频率最高的bigram为一个unigram。请实现下述函数，合并单词频次字典中的指定bigram为一个unigram，并更新单词频次字典。\n",
    "> <b>提示：</b>注意单词频次字典中，每个单词中的unigram都是由空格分隔开的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ba426043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_bpe_vocab_by_merging_bigram(bigram: Tuple[str, str], old_bpe_vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    在\"单词分词状态->频数\"的词典中，合并指定的bigram为单个unigram，最后返回新的词典。例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bigram = ('h', 'a'), old_bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha p p y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    Args:\n",
    "        bigram: Tuple[str, str] - 待合并的bigram\n",
    "        old_bpe_vocab: Dict[str, int] - 初始\"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - 合并后的\"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    new_bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】 ============================>>>\n",
    "    for item, count in old_bpe_vocab.items():\n",
    "        new_bpe_vocab[item.replace(bigram[0]+\" \"+bigram[1],bigram[0]+bigram[1])] = count\n",
    "\n",
    "    # <<<======================= END ========================\n",
    "\n",
    "    return new_bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9023c",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "357f3ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i </w>': 2,\n",
       " 'a m </w>': 1,\n",
       " 'ha p p y </w>': 1,\n",
       " 'ha v e </w>': 1,\n",
       " 'N </w>': 1,\n",
       " 'a p p l e s </w>': 1}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(('h', 'a'), test_bpe_vocab)\n",
    "test_new_bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2049c5",
   "metadata": {},
   "source": [
    "### 2.4 获取BPE分词器词表\n",
    "在充分合并bigram后，单词频次字典中剩下的所有unigram就构成了最终的词表tokens。在这里，我们希望在分词时贪婪地先匹配最长的（长度相同时最常见的）token，因此在实现下列函数时，请将BPE词表按照分词长度降序-出现频次的降序排序顺序返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "992438a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpe_tokens(bpe_vocab: Dict[str, int]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    根据\"单词分词状态->频数\"的词典，返回所得到的BPE分词token词表，并将该列表首先按照token长度降序排序返回，\n",
    "    token长度相同时再按照出现频次降序排序返回。例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```\n",
    "    [\n",
    "        ('i</w>', 2),\n",
    "        ('ha', 2),\n",
    "        ('pp', 2),\n",
    "        ('</w>', 5),\n",
    "        ('a', 2),\n",
    "        ('e', 2),\n",
    "        ('m', 1),\n",
    "        ('y', 1),\n",
    "        ('v', 1),\n",
    "        ('N', 1),\n",
    "        ('l', 1),\n",
    "        ('s', 1)\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        List[Tuple[str, int]] - BPE分词token和对应频数组成的List\n",
    "    \"\"\"\n",
    "\n",
    "    bpe_tokens_dict = {}\n",
    "    bpe_tokens = []\n",
    "    # TODO: 完成函数体【2分】 ============================>>>\n",
    "    for item,count in bpe_vocab.items():\n",
    "        characters = item.split(\" \")\n",
    "        for i in characters:\n",
    "            bpe_tokens_dict[i] = bpe_tokens_dict.get(i,0) + count\n",
    "    bpe_tokens = [(token, count) for token, count in bpe_tokens_dict.items()]\n",
    "    bpe_tokens.sort(key = lambda x: (-len(x[0])+x[0].count(\"</w>\")*3,-x[1]))    # 这里要把\"</w>\"看作一个字符\n",
    "\n",
    "    # <<<======================= END ========================\n",
    "\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668dff7",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0ba1197a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i</w>', 2),\n",
       " ('ha', 2),\n",
       " ('pp', 2),\n",
       " ('</w>', 5),\n",
       " ('a', 2),\n",
       " ('e', 2),\n",
       " ('m', 1),\n",
       " ('y', 1),\n",
       " ('v', 1),\n",
       " ('N', 1),\n",
       " ('l', 1),\n",
       " ('s', 1)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bpe_vocab = {\n",
    "    'i</w>': 2,\n",
    "    'a m </w>': 1,\n",
    "    'ha pp y </w>': 1,\n",
    "    'ha v e </w>': 1,\n",
    "    'N </w>': 1,\n",
    "    'a pp l e s </w>': 1\n",
    "}\n",
    "test_vocab = get_bpe_tokens(test_bpe_vocab)\n",
    "test_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eae0a3",
   "metadata": {},
   "source": [
    "### 2.5 对单词进行BPE分词\n",
    "获取了词表以后，就可以对单词做BPE分词了。按照上述的贪婪规则（先匹配最长的token，长度相同时先匹配最常见的token），请完成下列函数，对输入单词分词并打印出分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3c56995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bpe_tokenize(text: str, bpe_tokens: List[Tuple[str, int]]):\n",
    "    \"\"\"\n",
    "    根据按长度降序的BPE分词列表，将所给输入文本进行BPE分词，最后打印结果。\n",
    "    \n",
    "    首先对输入的文本（句子）做空白分词，获得单词序列，随后对于一个待BPE分词的单词，  \n",
    "    按照上述的贪婪规则从列表中寻找BPE分词进行子串匹配，  \n",
    "    若成功匹配，则对该子串左右的剩余部分递归地进行下一轮匹配，直到剩余部分长度为0，  \n",
    "    或者剩余部分无法匹配（该部分整体由`\"<unknown>\"`代替）。\n",
    "    \n",
    "    例1：  \n",
    "    输入 `text = \"shanghai\"`, `bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ]`  \n",
    "    最终打印 `\"sh an g hai </w>\"`\n",
    "\n",
    "    例2：  \n",
    "    输入 `text = \"SU7 in supermarket!\"`, `bpe_tokens=[\n",
    "        (\"per\", 30),\n",
    "        (\"are\", 10),\n",
    "        (\"su\", 20),\n",
    "        (\"N\", 50),\n",
    "    ]`  \n",
    "    最终打印 `\"su N <unknown> <unknown> su per <unknown>\"`\n",
    "\n",
    "    Args:\n",
    "        text: str - 待分词的单词\n",
    "        bpe_tokens: List[Tuple(str, int)] - BPE分词和对应频数组成的列表\n",
    "    \"\"\"\n",
    "    bpe_words = [word + \"</w>\" for word in white_space_tokenize([text])[0]]\n",
    "    def bpe_tokenize(sub_word: str) -> str:\n",
    "        # TODO: 使用递归函数，定义该分词过程【2分】 =============>>>\n",
    "        if len(sub_word) == 0: return \"\"    # 递归终止\n",
    "        for character in bpe_tokens:\n",
    "            pos = sub_word.find(character[0])\n",
    "            if pos != -1:\n",
    "                return bpe_tokenize(sub_word[:pos]) + \" \" + character[0] + \" \" + bpe_tokenize(sub_word[pos+len(character[0]):])\n",
    "        return \"<unknown>\"  # OOV\n",
    "        # <<<======================= END ========================\n",
    "\n",
    "    res = \" \".join([bpe_tokenize(word) for word in bpe_words])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41cd0d",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "05a1aece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sh  an  g  hai  </w> \n",
      " su  N <unknown> <unknown>  su  per <unknown>\n"
     ]
    }
   ],
   "source": [
    "print_bpe_tokenize(\"shanghai\", [(\"hai\", 1), (\"sh\", 1), (\"an\", 1), (\"</w>\", 1), (\"g\", 1)])\n",
    "print_bpe_tokenize(\"SU7 in supermarket!\", [(\"per\", 30), (\"are\", 10), (\"su\", 20), (\"N\", 50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70402",
   "metadata": {},
   "source": [
    "### 2.6 训练BPE分词器\n",
    "BPE分词器的训练是迭代地“找到频次最高的bigram-合并为unigram”的过程。请利用好上述编写的函数，对分词器做训练，直至词表达到指定规模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7bccd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_vocab(corpus: List[str], target_vocab_size: int) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    训练BPE分词直至词表达到指定大小，得到BPE分词token列表。\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 训练语料\n",
    "        target_vocab_size: int - 目标词表大小\n",
    "\n",
    "    Return:\n",
    "        List[Tuple[str, int]] - BPE分词token和对应频数组成的List\n",
    "    \"\"\"\n",
    "    bpe_vocab = build_bpe_vocab(corpus)\n",
    "    bpe_tokens = get_bpe_tokens(bpe_vocab)\n",
    "    print(\"初始BPE词典大小：\", len(bpe_tokens))\n",
    "    with tqdm(desc=\"训练BPE分词器\", total=target_vocab_size) as pbar:\n",
    "        while len(bpe_tokens) < target_vocab_size:\n",
    "            # TODO: 完成训练循环内的代码逻辑【2分】 ================>>>\n",
    "            biagram_freq = get_bigram_freq(bpe_vocab)   # 得到bigram频率\n",
    "            max_item = max(biagram_freq.items(), key=lambda x: x[1])    # bigram频率最高的两个token\n",
    "            bpe_vocab = refresh_bpe_vocab_by_merging_bigram(max_item[0],bpe_vocab)  # 合并token，更新bpe_vocab\n",
    "            bpe_tokens = get_bpe_tokens(bpe_vocab)  # 更新bpe_token\n",
    "            # <<<======================= END ========================\n",
    "\n",
    "            pbar.n = len(bpe_tokens)\n",
    "            pbar.refresh()\n",
    "\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea3ddd",
   "metadata": {},
   "source": [
    "### 测试BPE分词器的分词效果\n",
    "我们进行两次不同程度的训练：第一次训练仅简单将词表扩充至400，第二次训练将词表扩充至4096。观察一下，不同程度的训练对分词结果有什么影响？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c0cfdb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始BPE词典大小： 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练BPE分词器: 439it [00:05, 82.59it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始BPE词典大小： 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练BPE分词器: 4303it [00:12, 356.95it/s]                          \n"
     ]
    }
   ],
   "source": [
    "bpe_tokens_1 = train_bpe_vocab(training_corpus, 400)\n",
    "bpe_tokens_2 = train_bpe_vocab(training_corpus, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ba97201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 分词器的分词结果为：\n",
      " n  at  u  r  al  l  an  g  u  a  g  e  p  ro  c  e  s  sing</w> \n",
      " th  ec  l  ou  d  s  l  o  o  k  e  d  n  on  e  arer  th  an  wh  en  i  w  a  s  l  y  in  g  on  th  e  st  re  e  t</w> \n",
      " sh  an  g  h  ai  </w>   j  i  a  o</w>   t  on  g</w>   un  i  v  er  sit  y</w>   w  as</w>   f  oun  d  ed</w>   in</w>   N</w> \n",
      " the</w>   q  u  ic  k</w>   b  row  n  </w>   f  o  x  </w>   j  u  m  p  s</w>   o  v  er</w>   the</w>   l  a  z  y</w>   d  o  g</w> \n",
      " i  </w>   l  o  ve</w>   n  at  u  r  al</w>   l  an  g  u  a  g  e</w>   p  ro  c  e  s  sing</w> \n",
      "#2 分词器的分词结果为：\n",
      " n  at  ur  al  l  an  g  u  age  proc  e  s  sing</w> \n",
      " the  c  lou  d  s  lo  o  ke  d  none  are  r  than  wh  en  i  w  as  l  y  in  gon  the  stre  e  t</w> \n",
      " shan  ghai</w>   j  i  a  o</w>   t  on  g</w>   un  i  ver  sit  y</w>   was</w>   f  oun  ded</w>   in</w>   N</w> \n",
      " the</w>   qu  ic  k</w>   b  row  n</w>   f  o  x  </w>   j  u  m  p  s</w>   o  ver</w>   the</w>   l  a  z  y</w>   d  o  g</w> \n",
      " i</w>   lo  ve</w>   n  at  ur  al</w>   l  an  g  u  age</w>   proc  e  s  sing</w> \n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"naturallanguageprocessing\",\n",
    "    \"thecloudslookednonearerthanwheniwaslyingonthestreet\",\n",
    "    \"Shanghai Jiao Tong University was founded in 1896.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I love natural language processing!\"\n",
    "]\n",
    "print(f\"#1 分词器的分词结果为：\")\n",
    "for test_text in test_texts:\n",
    "    print_bpe_tokenize(test_text, bpe_tokens_1)\n",
    "print(f\"#2 分词器的分词结果为：\")\n",
    "for test_text in test_texts:\n",
    "    print_bpe_tokenize(test_text, bpe_tokens_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5f2a1",
   "metadata": {},
   "source": [
    "### 实验总结\n",
    "> TODO：请在这里填写实验总结。\n",
    "从头开始implement了WordPiece算法\n",
    "按照其定义逐步定义函数即可\n",
    "要注意\n",
    "1. 更新bigram的时候，要考虑单词本身的频次\n",
    "2. </w>的存在让计算字符串长度不能简单地使用len函数。要记得</w>只能看作一个字符"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
