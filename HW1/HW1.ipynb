{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5a31d058a6316b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业一：实现HMM中文分词和BPE英文分词\n",
    "姓名：\n",
    "\n",
    "学号："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a16ed",
   "metadata": {},
   "source": [
    "## 任务一：HMM模型用于中文分词\n",
    "\n",
    "任务一评分标准：\n",
    "1. 共有8处TODO需要填写，共10分。\n",
    "2. **可编辑代码区域仅限定在TODO的范围内，不允许自行修改其他部分代码。**\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77db9",
   "metadata": {},
   "source": [
    "导入HMM参数，初始化所需的起始概率矩阵、转移概率矩阵、发射概率矩阵，并将它们转换为<b>对数形式</b>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hmm_parameters.pkl\", \"rb\") as f:\n",
    "    hmm_parameters = pickle.load(f)\n",
    "\n",
    "# 非断字（B）为第0行，断字（I）为第1行\n",
    "# 发射概率矩阵中，词典大小为65536，以汉字的Unicode码点（一个整数值）作为行索引\n",
    "start_prob_log = np.log(hmm_parameters[\"start_prob\"])  # shape(2,)\n",
    "trans_matrix_log = np.log(hmm_parameters[\"trans_mat\"])  # shape(2, 2)\n",
    "emission_matrix_log = np.log(hmm_parameters[\"emission_mat\"])  # shape(2, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070152",
   "metadata": {},
   "source": [
    "定义待处理的句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87219e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 将your_name中的xxx替换为你的姓名\n",
    "your_name = \"xxx\"\n",
    "\n",
    "input_sentence = f\"{your_name}是一名优秀的学生\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035cbc7",
   "metadata": {},
   "source": [
    "### 1.1 实现Viterbi算法\n",
    "实现Viterbi算法，并以此进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sent_orig: str, start_prob_log: np.ndarray, trans_mat_log: np.ndarray, emission_mat_log: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Viterbi算法进行中文分词。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob_log: numpy.ndarray - 起始对数概率矩阵\n",
    "        trans_mat_log: numpy.ndarray - 转移对数概率矩阵\n",
    "        emission_mat_log: numpy.ndarray - 发射对数概率矩阵\n",
    "\n",
    "    Return:\n",
    "        str - 中文分词的结果\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种标注（B/I）的最大对数概率值\n",
    "    dp = np.empty((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # `path`用来储存最大概率对应的上步B/I选择\n",
    "    #  例如 path[1][7] == 1 意味着第8个（从1开始计数）字符标注I对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[0][5] == 1 意味着第6个字符标注B对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[1][1] == 0 意味着第2个字符标注I对应的最大概率，其前一步的隐状态为0（B）\n",
    "    path = np.zeros((2, len(sent_ord)), dtype=int)\n",
    "\n",
    "    # [1] TODO: 第一个位置的最大概率值计算【1分】 =================>>>\n",
    "    pass\n",
    "\n",
    "    # [1] <<<======================= END ==========================\n",
    "\n",
    "    # [2] TODO: 其余位置的最大概率值计算（填充dp和path矩阵）【2分】 =====>>>\n",
    "    pass\n",
    "\n",
    "    # [2] <<<======================= END ==========================\n",
    "\n",
    "    #  `labels`用来储存每个位置最有可能的隐状态\n",
    "    labels = [0 for _ in range(len(sent_ord))]\n",
    "\n",
    "    # [3] TODO: 计算labels每个位置上的值（填充labels矩阵）【1分】 ====>>>\n",
    "    pass\n",
    "\n",
    "    # [3] <<<======================= END ==========================\n",
    "\n",
    "    #  根据labels生成切分好的字符串\n",
    "    sent_split = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            sent_split += [sent_ord[idx], ord(\"/\")]\n",
    "        else:\n",
    "            sent_split += [sent_ord[idx]]\n",
    "    sent_split_str = \"\".join([chr(x) for x in sent_split])\n",
    "\n",
    "    return sent_split_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b7bd0",
   "metadata": {},
   "source": [
    "你可以用下述句子测试Viterbi算法的实现：若实现正确，则输出结果应该是`我/爱上/海交/通大/学的/自然/语言/处理/`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd367283",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"我爱上海交通大学的自然语言处理\"\n",
    "print(\"Viterbi算法分词结果：\", viterbi(test_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc86ae2",
   "metadata": {},
   "source": [
    "检查无误后运行下方单元格，对`input_sentence`做分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Viterbi算法分词结果：\", viterbi(input_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcafdb",
   "metadata": {},
   "source": [
    "### 1.2 实现前向算法\n",
    "实现前向算法，计算该句子的对数概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob_by_forward(sent_orig: str, start_prob_log: np.ndarray, trans_mat_log: np.ndarray, emission_mat_log: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    前向算法，计算输入中文句子的对数概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob_log: numpy.ndarray - 起始对数概率矩阵\n",
    "        trans_mat_log: numpy.ndarray - 转移对数概率矩阵\n",
    "        emission_mat_log: numpy.ndarray - 发射对数概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 对数概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，到该位置为止的句子的对数概率\n",
    "    dp = np.empty((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # [1] TODO: 初始位置概率的计算【1分】 ==========================>>>\n",
    "    pass\n",
    "\n",
    "    # [1] <<<======================= END ==========================\n",
    "\n",
    "    ans = None\n",
    "    # [2] TODO: 先计算其余位置的概率（填充dp矩阵），然后返回对数概率值【2分】 =====>>>\n",
    "    pass\n",
    "\n",
    "    # [2] <<<======================= END ==========================\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59533cd8",
   "metadata": {},
   "source": [
    "### 1.3 实现后向算法\n",
    "实现后向算法，计算该句子的对数概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e898306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob_by_backward(sent_orig: str, start_prob_log: np.ndarray, trans_mat_log: np.ndarray, emission_mat_log: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    后向算法，计算输入中文句子的对数概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob_log: numpy.ndarray - 起始对数概率矩阵\n",
    "        trans_mat_log: numpy.ndarray - 转移对数概率矩阵\n",
    "        emission_mat_log: numpy.ndarray - 发射对数概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 对数概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，从结尾到该位置为止的句子的概率\n",
    "    dp = np.empty((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # [1] TODO: 终末位置概率的初始化【1分】 =================>>>\n",
    "    pass\n",
    "\n",
    "    # [1] <<<======================= END =====================\n",
    "\n",
    "    ans = None\n",
    "    # [2] TODO: 先计算其余位置的概率（填充dp矩阵），然后返回概率值【2分】 =====>>>\n",
    "    pass\n",
    "\n",
    "    # [2] <<<======================= END ==========================\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209660a",
   "metadata": {},
   "source": [
    "如果前向算法与后向算法的实现正确，下面的测试句子所给出的两种算法概率应当几乎相等，约为`-99.9266`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33378f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"我爱上海交通大学的自然语言处理\"\n",
    "print(\"前向算法概率：\", compute_logprob_by_forward(test_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))\n",
    "print(\"后向算法概率：\", compute_logprob_by_backward(test_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a386f49",
   "metadata": {},
   "source": [
    "现在计算`input_sentence`的句子概率值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26101d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"前向算法概率：\", compute_logprob_by_forward(input_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))\n",
    "print(\"后向算法概率：\", compute_logprob_by_backward(input_sentence, start_prob_log, trans_matrix_log, emission_matrix_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49866453",
   "metadata": {},
   "source": [
    "> 如果你的名字含有生僻字，分词结果以及计算出的句子概率值可能会很“奇怪”。思考一下这是为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803f6a1a465dd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 实验总结\n",
    "> TODO：请在这里填写实验总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994be6e",
   "metadata": {},
   "source": [
    "## 任务二：BPE算法用于英文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc4775",
   "metadata": {},
   "source": [
    "任务二评分标准：\n",
    "\n",
    "1. 共有6处TODO需要填写，共10分。\n",
    "2. **可编辑代码区域仅限定在TODO的范围内，不允许自行修改其他部分代码。**\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02463b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3951dc",
   "metadata": {},
   "source": [
    "导入英语语料库数据。利用这些自然语料，我们将构建一个BPE分词器，对输入文本进行tokenize。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_2007_en.txt\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()))     # List[str]\n",
    "for line in training_corpus[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732502a",
   "metadata": {},
   "source": [
    "### 2.1 构建单词频次字典\n",
    "首先将语料中的句子以空格切分成单词，然后将单词拆分成字母加`</w>`的形式，例如`apple`将变为`a p p l e </w>`。请编写相应函数构建BPE算法需要用到的初始状态词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf823e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "def white_space_tokenize(corpus: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    先正则化（字母转小写、数字转为N、除去标点符号），然后以空格分词语料中的句子，例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到 `[[\"i\", \"am\", \"happy\"], [\"i\", \"have\", \"N\", \"apples\"]]`\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        List[List[str]] - 二维List，内部的List由每个句子的单词str构成\n",
    "\n",
    "    【辅助函数，该函数无需改动】\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [list(filter(lambda token: len(token) > 0, _splitor_pattern.split(_digit_pattern.sub(\"N\", sentence.lower())))) for sentence in corpus]\n",
    "    return tokeneds\n",
    "\n",
    "def build_bpe_vocab(corpus: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    获取语料库中的所有单词，将单词每个字母以空格隔开、结尾加上</w>（单词终止符）后，构建带频数的字典。例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = white_space_tokenize(corpus)\n",
    "\n",
    "    bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体【2分】 =============================>>>\n",
    "    pass\n",
    "\n",
    "    # <<<======================= END ========================\n",
    "\n",
    "    return bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1a932",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cce7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bpe_vocab = build_bpe_vocab([\"I am happy.\", \"I have 10 apples!\"])\n",
    "test_bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d25245",
   "metadata": {},
   "source": [
    "### 2.2 构建bigram频次字典\n",
    "单词频次字典中，每个键都是由空格分隔开的unigram组成的字符串。请统计这些unigram所组成的所有bigram的频次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_freq(bpe_vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    统计\"单词分词状态->频数\"的词典中，各bigram的频次（假设该词典中，各个unigram以空格间隔），例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        ('i', '</w>'): 2,\n",
    "        ('a', 'm'): 1,\n",
    "        ('m', '</w>'): 1,\n",
    "        ('h', 'a'): 2,\n",
    "        ('a', 'p'): 2,\n",
    "        ('p', 'p'): 2,\n",
    "        ('p', 'y'): 1,\n",
    "        ('y', '</w>'): 1,\n",
    "        ('a', 'v'): 1,\n",
    "        ('v', 'e'): 1,\n",
    "        ('e', '</w>'): 1,\n",
    "        ('N', '</w>'): 1,\n",
    "        ('p', 'l'): 1,\n",
    "        ('l', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', '</w>'): 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[Tuple[str, str], int] - \"bigram->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_freq = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】 =============================>>>\n",
    "    pass\n",
    "\n",
    "    # <<<======================= END ========================\n",
    "\n",
    "    return bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1ff1b",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bigram_freq = get_bigram_freq(test_bpe_vocab)\n",
    "test_bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d313f",
   "metadata": {},
   "source": [
    "### 2.3 合并bigram\n",
    "BPE算法的每轮迭代都会合并出现频率最高的bigram为一个unigram。请实现下述函数，合并单词频次字典中的指定bigram为一个unigram，并更新单词频次字典。\n",
    "> <b>提示：</b>注意单词频次字典中，每个单词中的unigram都是由空格分隔开的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba426043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_bpe_vocab_by_merging_bigram(bigram: Tuple[str, str], old_bpe_vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    在\"单词分词状态->频数\"的词典中，合并指定的bigram为单个unigram，最后返回新的词典。例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bigram = ('h', 'a'), old_bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha p p y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    Args:\n",
    "        bigram: Tuple[str, str] - 待合并的bigram\n",
    "        old_bpe_vocab: Dict[str, int] - 初始\"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - 合并后的\"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    new_bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】 ============================>>>\n",
    "    pass\n",
    "\n",
    "    # <<<======================= END ========================\n",
    "\n",
    "    return new_bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9023c",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(('h', 'a'), test_bpe_vocab)\n",
    "test_new_bpe_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2049c5",
   "metadata": {},
   "source": [
    "### 2.4 获取BPE分词器词表\n",
    "在充分合并bigram后，单词频次字典中剩下的所有unigram就构成了最终的词表tokens。在这里，我们希望在分词时贪婪地先匹配最长的（长度相同时最常见的）token，因此在实现下列函数时，请将BPE词表按照分词长度降序-出现频次的降序排序顺序返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992438a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpe_tokens(bpe_vocab: Dict[str, int]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    根据\"单词分词状态->频数\"的词典，返回所得到的BPE分词token词表，并将该列表首先按照token长度降序排序返回，\n",
    "    token长度相同时再按照出现频次降序排序返回。例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```\n",
    "    [\n",
    "        ('i</w>', 2),\n",
    "        ('ha', 2),\n",
    "        ('pp', 2),\n",
    "        ('</w>', 5),\n",
    "        ('a', 2),\n",
    "        ('e', 2),\n",
    "        ('m', 1),\n",
    "        ('y', 1),\n",
    "        ('v', 1),\n",
    "        ('N', 1),\n",
    "        ('l', 1),\n",
    "        ('s', 1)\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        List[Tuple[str, int]] - BPE分词token和对应频数组成的List\n",
    "    \"\"\"\n",
    "\n",
    "    bpe_tokens = []\n",
    "    # TODO: 完成函数体【2分】 ============================>>>\n",
    "    pass\n",
    "\n",
    "    # <<<======================= END ========================\n",
    "\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668dff7",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bpe_vocab = {\n",
    "    'i</w>': 2,\n",
    "    'a m </w>': 1,\n",
    "    'ha pp y </w>': 1,\n",
    "    'ha v e </w>': 1,\n",
    "    'N </w>': 1,\n",
    "    'a pp l e s </w>': 1\n",
    "}\n",
    "test_vocab = get_bpe_tokens(test_bpe_vocab)\n",
    "test_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eae0a3",
   "metadata": {},
   "source": [
    "### 2.5 对单词进行BPE分词\n",
    "获取了词表以后，就可以对单词做BPE分词了。按照上述的贪婪规则（先匹配最长的token，长度相同时先匹配最常见的token），请完成下列函数，对输入单词分词并打印出分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bpe_tokenize(text: str, bpe_tokens: List[Tuple[str, int]]):\n",
    "    \"\"\"\n",
    "    根据按长度降序的BPE分词列表，将所给输入文本进行BPE分词，最后打印结果。\n",
    "    \n",
    "    首先对输入的文本（句子）做空白分词，获得单词序列，随后对于一个待BPE分词的单词，  \n",
    "    按照上述的贪婪规则从列表中寻找BPE分词进行子串匹配，  \n",
    "    若成功匹配，则对该子串左右的剩余部分递归地进行下一轮匹配，直到剩余部分长度为0，  \n",
    "    或者剩余部分无法匹配（该部分整体由`\"<unknown>\"`代替）。\n",
    "    \n",
    "    例1：  \n",
    "    输入 `text = \"shanghai\"`, `bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ]`  \n",
    "    最终打印 `\"sh an g hai </w>\"`\n",
    "\n",
    "    例2：  \n",
    "    输入 `text = \"SU7 in supermarket!\"`, `bpe_tokens=[\n",
    "        (\"per\", 30),\n",
    "        (\"are\", 10),\n",
    "        (\"su\", 20),\n",
    "        (\"N\", 50),\n",
    "    ]`  \n",
    "    最终打印 `\"su N <unknown> <unknown> su per <unknown>\"`\n",
    "\n",
    "    Args:\n",
    "        text: str - 待分词的单词\n",
    "        bpe_tokens: List[Tuple(str, int)] - BPE分词和对应频数组成的列表\n",
    "    \"\"\"\n",
    "    bpe_words = [word + \"</w>\" for word in white_space_tokenize([text])[0]]\n",
    "    def bpe_tokenize(sub_word: str) -> str:\n",
    "        # TODO: 使用递归函数，定义该分词过程【2分】 =============>>>\n",
    "        pass\n",
    "\n",
    "        # <<<======================= END ========================\n",
    "\n",
    "    res = \" \".join([bpe_tokenize(word) for word in bpe_words])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41cd0d",
   "metadata": {},
   "source": [
    "检查你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bpe_tokenize(\"shanghai\", [(\"hai\", 1), (\"sh\", 1), (\"an\", 1), (\"</w>\", 1), (\"g\", 1)])\n",
    "print_bpe_tokenize(\"SU7 in supermarket!\", [(\"per\", 30), (\"are\", 10), (\"su\", 20), (\"N\", 50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70402",
   "metadata": {},
   "source": [
    "### 2.6 训练BPE分词器\n",
    "BPE分词器的训练是迭代地“找到频次最高的bigram-合并为unigram”的过程。请利用好上述编写的函数，对分词器做训练，直至词表达到指定规模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_vocab(corpus: List[str], target_vocab_size: int) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    训练BPE分词直至词表达到指定大小，得到BPE分词token列表。\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 训练语料\n",
    "        target_vocab_size: int - 目标词表大小\n",
    "\n",
    "    Return:\n",
    "        List[Tuple[str, int]] - BPE分词token和对应频数组成的List\n",
    "    \"\"\"\n",
    "    bpe_vocab = build_bpe_vocab(corpus)\n",
    "    bpe_tokens = get_bpe_tokens(bpe_vocab)\n",
    "    print(\"初始BPE词典大小：\", len(bpe_tokens))\n",
    "    with tqdm(desc=\"训练BPE分词器\", total=target_vocab_size) as pbar:\n",
    "        while len(bpe_tokens) < target_vocab_size:\n",
    "            # TODO: 完成训练循环内的代码逻辑【2分】 ================>>>\n",
    "            pass\n",
    "\n",
    "            # <<<======================= END ========================\n",
    "\n",
    "            pbar.n = len(bpe_tokens)\n",
    "            pbar.refresh()\n",
    "\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea3ddd",
   "metadata": {},
   "source": [
    "### 测试BPE分词器的分词效果\n",
    "我们进行两次不同程度的训练：第一次训练仅简单将词表扩充至400，第二次训练将词表扩充至4096。观察一下，不同程度的训练对分词结果有什么影响？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cfdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokens_1 = train_bpe_vocab(training_corpus, 400)\n",
    "bpe_tokens_2 = train_bpe_vocab(training_corpus, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"naturallanguageprocessing\",\n",
    "    \"thecloudslookednonearerthanwheniwaslyingonthestreet\",\n",
    "    \"Shanghai Jiao Tong University was founded in 1896.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I love natural language processing!\"\n",
    "]\n",
    "print(f\"#1 分词器的分词结果为：\")\n",
    "for test_text in test_texts:\n",
    "    print_bpe_tokenize(test_text, bpe_tokens_1)\n",
    "print(f\"#2 分词器的分词结果为：\")\n",
    "for test_text in test_texts:\n",
    "    print_bpe_tokenize(test_text, bpe_tokens_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5f2a1",
   "metadata": {},
   "source": [
    "### 实验总结\n",
    "> TODO：请在这里填写实验总结。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
